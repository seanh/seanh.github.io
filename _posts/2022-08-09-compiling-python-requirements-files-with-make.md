---
draft: true
sitemap: false
robots: noindex, nofollow
---

Compiling Python Requirements Files with GNU `make`
===================================================

* This unordered list will be replaced with a table of contents.
{:toc}

What's a requirements file?
---------------------------

A requirements file is a file containing the list of packages that an app
depends on. For example:

    # requirements.in
    certifi
    gunicorn
    pyramid
    pyramid-jinja2
    requests

Each line is the name of a package from <https://pypi.org/>.
You can use [pip](https://pip.pypa.io/) to install the packages from a
requirements file into a virtual environment like this:

```terminal
$ pip install -r requirements.in
```

The pip documentation contains the
[docs for the requirements file format](https://pip.pypa.io/en/stable/reference/requirements-file-format).

<details markdown="1">
<summary>Virtual environments</summary>
A "virtual environment" is an isolated Python environment that you can install
packages into without messing up (or being affected by) your host system's main
Python environment.
You always want to be using these in local development, on CI, and when deploying web apps to servers.
Real Python has a [good primer on them](https://realpython.com/python-virtual-environments-a-primer/).
</details>

<details markdown="1">
<summary>Requirements files versus <code>pyproject.toml</code></summary>
TODO. References:

* https://blog.miguelgrinberg.com/post/the-package-dependency-blues
* https://caremad.io/posts/2013/07/setup-vs-requirement/
* https://www.b-list.org/weblog/2020/jan/05/packaging/
* https://www.b-list.org/weblog/2022/may/13/boring-python-dependencies/
</details>

### Pinning your requirements

<details markdown="1">
<summary>Why should you pin your requirements?</summary>
TODO
</details>

We use the `pip-compile` command from the
[pip-tools](https://pip-tools.readthedocs.io/) package to create pinned
`requirements.txt` files from un-pinned `requirements.in` files.

For example this command will create a `requirements.txt` file specifying the
newest possible versions of all the packages in `requirements.in`:

```terminal
$ pip-compile requirements.in
```

The `requirements.txt` file that will be created will contain not only the app's **direct dependencies**
(that are listed in `requirements.in`) but also its **indirect dependencies**:
the dependencies of the direct dependencies, and the dependencies of those
dependencies:

    #
    # This file is autogenerated by pip-compile with python 3.10
    # To update, run:
    #
    #    pip-compile requirements.in
    #
    certifi==2022.6.15
        # via
        #   -r requirements.in
        #   requests
    charset-normalizer==2.1.0
        # via requests
    gunicorn==20.1.0
        # via -r requirements.in
    hupper==1.10.3
        # via pyramid
    ...

You can install a `pip-compile`'d `requirements.txt` file using
[pip-sync](https://pip-tools.readthedocs.io/en/latest/#example-usage-for-pip-sync)
(which comes with the same `pip-tools` package that contains `pip-compile`).
This creates a **reproducible virtual environment**: creating a virtualenv and
installing a pinned `requirements.txt` file will always result in a virtualenv
containing all the same versions of the same packages in local development, on
CI, and in production:

```terminal
$ pip-sync requirements.txt
```

<small>(Technically I believe that this can result in different packages getting
installed on different operating systems, CPU architectures, etc. But at least
if you use `pip-sync` to install the same `pip-compile`'d requirements file in
the same environment it should always produce the same result.)</small>

### Requirements files need to be compiled with the right Python version

Compiling a `requirements.in` file with `pip-compile` can produce different
`requirements.txt` files for different versions of Python.
This happens because a Python package's dependencies can contain
[environment markers](https://peps.python.org/pep-0508/#environment-markers) that make the
dependency conditional on the Python version. For example to
depend on [argparse](https://pypi.org/project/argparse/) but only in Python 2.7 or earlier
a package could put this in its `pyproject.toml` file:

```toml
dependencies = [
    'argparse;python_version<"2.7"',
]
```

Any of an app's direct or indirect dependencies might have such a conditional
dependency, they're not rare.

When you compile a `requirements.in` file with `pip-compile` it resolves
conditional dependencies according to the Python version that `pip-compile` is
run with. So if you run `pip-compile` with Python 2.7 or earlier you'll get
`argparse` in your `requirements.txt` file but if you run it with Python 3 you
won't.

At Hypothesis we handle this by using [pyenv](https://github.com/pyenv/pyenv/)
to run `pip-compile` with the right version of Python:

```terminal
$ pyenv exec pip install pip-tools
$ pyenv exec pip-compile requirements.in
```

<details markdown="1">
<summary>What's pyenv?</summary>
Briefly, pyenv is a Python version manager.
It lets you:

1. Install multiple Python versions at once with commands like
   `pyenv install 3.10.4` or `pyenv install 3.8.13`
  (they all get installed into a `~/.pyenv/versions/` directory)
2. Specify the Python version for a project by putting a `.python-version` file
   in the project's directory, like this:

   ```
   # .python-version
   3.10.4
   ```
3. Run a command using the project's Python version by running
   `pyenv exec <COMMAND>`, for example:

   ```terminal
   $ pyenv exec pip-compile requirements/prod.in
   ```

   `pyenv exec` will read the current working directory's `.python-version`
   file and run `pip-compile` with that version of Python.
   (You need to have first installed `pip-tools` in that version of Python
   with `pyenv exec pip install pip-tools`.)

You wouldn't normally need `pyenv exec`:
you'd normally have added pyenv's "shims" to your shell's `PATH` as part of following
pyenv's installation instructions so commands like `pip` or `pip-compile` will
run through pyenv on their own even without the `pyenv exec` prefix (after installing `pip-tools`
you may need to run `pyenv rehash` to get this working).
But running the commands with the `pyenv exec` prefix means that they'll always run via
pyenv even if the user has [installed pyenv without shims](https://github.com/pyenv/pyenv/#using-pyenv-without-shims).

There's more to pyenv, see [pyenv's README](https://github.com/pyenv/pyenv/) for docs.
</details>

<details markdown="1">
<summary>Better Python lockfile formats</summary>
Pinned `requirements.txt` files are quite primitive compared to the lockfile
formats available in other languages like Node, Go and Rust. Not only
do separate requirements files need to be compiled for each version of
Python, in some cases you might need to compile separate requirements files for
different operating systems, operating system versions, CPU architectures, and
other environment markers that might appear in your dependencies.

As far as I know a better standard requirements file format for Python is not
on the horizon. [PEP 665](https://peps.python.org/pep-0665/) was rejected.
The [thread on discuss.python.org](https://discuss.python.org/t/structured-exchangeable-lock-file-format-requirements-txt-2-0/876)
seems to have dried up.

Aside from `pip-compile` several other tools also define their own Python
lockfile formats, including [Pipenv](https://pipenv.pypa.io/) and
[Poetry](https://python-poetry.org/).  But these are custom third-party
lockfile formats, they aren't standard and the community hasn't coalesced
around any one of them.
</details>

### Hypothesis apps have multiple requirements files

At Hypothesis each one of our apps actually has a set of multiple requirements
files, all in the `requirements/` directory:

```
requirements/
  prod.in
  prod.txt
  dev.in
  dev.txt
  format.in
  format.txt
  lint.in
  lint.txt
  tests.in
  tests.txt
  coverage.in
  coverage.txt
  functests.in
  functests.txt
```

This has to do with the particular way that we use [tox](https://tox.wiki/)
(a Python virtualenv manager) at Hypothesis, but:

Each `requirements/*.txt` file specifies the contents of a separate virtualenv
that tox will create in its `.tox` directory. For example when you run `make format`
this calls `tox -e format` which creates a virtualenv at `.tox/format` (if it
doesn't already exist) and our `tox.ini` file then tells `tox` to install
`requirements/format.txt` into that virtualenv (if it isn't already installed)
and then to run our code formatting commands
([`black`](https://black.readthedocs.io/) and
[`isort`](https://pycqa.github.io/isort/)) in the virtualenv.  `format.txt`
only contains the packages necessary for formatting the code and their
dependencies. Nothing else needs to be installed in the `format` virtualenv
because that virtualenv is only used for running the code formatters.

Similarly `lint.txt` lists the dependencies for running the linters (`make
lint`), `tests.txt` lists the dependencies for running the unit tests (`make
test`), `dev.txt` lists the dependencies for running the local development
server (`make dev`), and so on.

`prod.txt` lists the app's production dependencies: this is what actually gets
installed into the app's Docker image before it gets deployed to production.

<details markdown="1">
<summary>Why do we use separate virtualenvs?</summary>
There are a few reasons why we separate things into different virtualenvs
rather than just installing everything into a single development environment
and running all commands in that environment:

* It avoids dependency conflicts between different development tools.

  We're not worried about dependency conflicts between Black and
  [Pylint](https://pylint.pycqa.org/) because we don't install those two tools
  into the same virtualenv.

* It means we can use tox to run everything in parallel (which is much faster)
  with a command like `tox --parallel format,lint,test,coverage,functests`.
  Our `make sure` command does this.

* It's easier.
  When you use tox in the way that we do separate virtualenvs is just what
  happens naturally. It allows all the details (for example the exact `black`,
  `isort`, `pylint`, `pytest` and `coverage` commands) to live in the `tox.ini`
  file and provides simple commands like `tox -e format` (to run the code
  formatters) or `tox -e lint` (to run the linters). It'd actually be more
  complex to get `tox` to create one big virtualenv to run all the commands in (and
  then it wouldn't be able to run them in parallel anymore).

Virtualenvs are cheap. `tox` handles creating them all for you and using lots
of separate ones enables some nice features.
</details>

How to make a file with GNU `make`
----------------------------------

Now that we know about Python requirements files and some of the complexities
involved in compiling them we're ready to meet [GNU
`make`](https://www.gnu.org/software/make/), the tool that we'll be using to
build a requirements file compiler. Before we can get started we need to learn
a couple of `make` basics.

`make` needs a file named `Makefile` in the current working directory to
specify the recipes for it to use to make different files. Here's a simple
`Makefile` that tells `make` how to make a `foo.txt` file by running `touch
foo.txt`:

```make
# Makefile
foo.txt:
	touch foo.txt
```

With this `Makefile` in the current directory if you run `make foo.txt` it'll run the `touch foo.txt` command which will
create the `foo.txt` file.  `make` helpfully prints out each command before
running it:

```terminal
$ make foo.txt
touch foo.txt
$ ls foo.txt  # An empty foo.txt file has been created.
foo.txt
```

If you now re-run `make foo.txt` it won't re-run the recipe because it sees that
`foo.txt` already exists:

```terminal
$ make foo.txt
make: 'foo.txt' is up to date.
```

If you delete `foo.txt` and then re-run `make foo.txt` then it _will_ remake
the file for you:

```terminal
$ rm foo.txt
$ make foo.txt
touch foo.txt
```

Prerequisites in `make`
-----------------------

Let's tell `make` how to make a `bar.txt` output file that depends on a `bar.in` input file.
In `make`'s terms `bar.in` is called a **prerequisite** of `bar.txt`:

```make
# Makefile
bar.txt: bar.in
	cp bar.in bar.txt
```

Now if you run `make bar.txt` it'll make the `bar.txt` file by running the `cp
bar.in bar.txt` command. You have to create `bar.in` yourself first:

```terminal
$ echo foobar > bar.in  # Create a bar.in file containing "foobar".
$ make bar.txt
cp bar.in bar.txt
$ cat bar.txt  # bar.txt has been created by copying bar.in.
foobar
```

As before if you re-run `make bar.txt` it won't re-run the recipe because
it'll see that `bar.txt` already exists: `make: 'bar.txt' is up to date`.

But because `bar.txt` depends on `bar.in`, `make bar.txt` *will* remake the
file if `bar.in` has changed:

```terminal
$ echo changed >> bar.in
$ make bar.txt
cp bar.in bar.txt
$ cat bar.txt
foobar
changed
```

`make` uses file mtimes to decide whether a prerequisite (`bar.in`) is newer
than the target (`bar.txt`) and therefore whether the target needs to be
remade. So instead of modifying `bar.in` you can also just run `touch bar.in`
(which updates `bar.in`'s mtime without changing its contents) and then `make
bar.txt` will remake `bar.txt`. Alternatively you can use the `--assume-new`
argument to `make` to tell it to pretend that `bar.in`'s mtime has changed even
if it hasn't, this will always remake `bar.txt`:

```terminal
$ make --assume-new=bar.in bar.txt
```

A target can have more than one prerequisite. This creates a `baz.txt` file
from three prerequisites `one.in`, `two.in` and `three.in`:

```make
# Makefile
baz.txt: one.in two.in three.in
	cat one.in two.in three.in > baz.txt
```

The target will be remade if *any* of its prerequisites have changed:

```terminal
$ echo one > one.in
$ echo two > two.in
$ echo three > three.in
$ make baz.txt
cat one.in two.in three.in > baz.txt
$ cat baz.txt
one
two
three
$ echo changed >> two.in
$ make baz.txt
cat one.in two.in three.in > baz.txt
$ cat baz.txt
one
two
changed
three
```

How to install `pip-compile` with `make`
----------------------------------------

Now that we know about make targets and prerequisites we can begin writing a
`Makefile` to compile Python requirements files.

Before we can compile requirements files with `pip-compile` we first need to
install `pip-tools` (the Python package that contains `pip-compile`) so that's
the first thing we'll get `make` to do for us.

```make
# Makefile
# Save the paths to pip and pip-compile in variables.
python_bin_dir := $(shell pyenv root)/versions/$(shell pyenv local)/bin
pip := $(python_bin_dir)/pip
pip_compile := $(python_bin_dir)/pip-compile

# Tell make how to install pip-compile.
$(pip_compile):
	$(pip) install pip-tools > /dev/null 2>&1
```

With this in your `Makefile` you can run `make $(pyenv root)/versions/$(pyenv local)/bin/pip-compile`
and it'll install `pip-compile` into pyenv's Python version for the current
working directory if it isn't already installed:

```terminal
$ make $(pyenv root)/versions/$(pyenv local)/bin/pip-compile
/home/seanh/.pyenv/versions/3.10.4/bin/pip install pip-tools > /dev/null 2>&1
$ make $(pyenv prefix)/bin/pip-compile
make: '/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile' is up to date.
```

There's a few things going on here:

1. The `:=` operator assigns a variable in `make`. Variables assigned with
   `:=` are expanded once when the variable is defined (alternatively you can
   assign a variable with `=` and it'll be re-expanded each time it's
   referenced, but that's not what we want here). See [How to Use
   Variables](https://www.gnu.org/software/make/manual/make.html#Using-Variables)
   in the `make` manual for the full details.

2. `$(shell <COMMAND>)` runs an external command and evaluates to the
   output of that command.  See [The `shell`
   Function](https://www.gnu.org/software/make/manual/make.html#Shell-Function)
   in the `make` manual.

3. We're using the `pyenv root` and `pyenv local` commands to define the `$(python_bin_dir)` variable:

   ```make
   python_bin_dir := $(shell pyenv root)/versions/$(shell pyenv local)/bin
   ```

   `pyenv root` is a pyenv command that returns the path to the pyenv
   directory, for example `/home/seanh/.pyenv`.

   `pyenv local` outputs the current working directory's local Python version
   from its `.python-version` file, for example: `3.10.4`.

   So the value of the `$(python_bin_dir)` variable ends up being something
   like `/home/seanh/.pyenv/versions/3.10.4/bin`, depending on what Python
   version is in your `.python-version` file.

   We then use `$(python_bin_dir)` to save the absolute paths to the `pip` and
   `pip-compile` binaries in `$(pip)` and `$(pip_compile)` variables:

   ```make
   pip := $(python_bin_dir)/pip
   pip_compile := $(python_bin_dir)/pip-compile
   ```

   It would be simpler to just run `pyenv exec pip ...` and `pyenv exec
   pip-compile ...` but `pyenv exec` might use a different Python version if
   the user has set a `PYENV_VERSION` environment variable (for example by
   using the `pyenv shell` command). Absolute paths will always use the
   directory's local Python version.

4. Finally, we use the `$(pip)` and `$(pip_compile)` variables to define a rule
   for installing `pip-compile`:

   ```make
   $(pip_compile):
   	$(pip) install pip-tools > /dev/null 2>&1
   ```

How to make a requirements file with `make`
-------------------------------------------

Now that we have a target for installing `pip-compile` we can write
another target to make the `prod.txt` file:

```make
# Makefile
# Save the paths to pip and pip-compile in variables.
python_bin_dir := $(shell pyenv root)/versions/$(shell pyenv local)/bin
pip := $(python_bin_dir)/pip
pip_compile := $(python_bin_dir)/pip-compile

# Tell make how to install pip-compile.
$(pip_compile):
	$(pip) install pip-tools > /dev/null 2>&1

# Tell make how to make requirements/prod.txt.
requirements/prod.txt: $(pip_compile)
	$(pip_compile) requirements/prod.in > /dev/null 2>&1
```

The `requirements/prod.txt` target depends on the `$(pip_compile)` target as a
prerequisite so `make` will install `pip-tools` (if it isn't installed already)
before running the `pip-compile` command:

```terminal
$ make requirements/prod.txt
/home/seanh/.pyenv/versions/3.10.4/bin/pip install pip-tools > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/prod.in > /dev/null 2>&1
$ cat requirements/prod.txt
# This file is autogenerated by pip-compile with python 3.10
# To update, run:
#
#    pip-compile requirements/prod.in
#
aiohttp==3.8.1
    # via -r requirements/prod.in
aiosignal==1.2.0
    # via aiohttp
alembic==1.8.1
    # via -r requirements/prod.in
...
```

As before if you re-run `make requirements/prod.txt` it will see that
`prod.txt` already exists and won't re-run the recipe.

If `pip-compile` is already installed `make` will see that and just skip
straight to the `pip-compile` command:

```terminal
$ rm requirements/prod.txt
$ make requirements/prod.txt
pyenv exec pip-compile requirements/prod.in > /dev/null 2>&1
```

Making `prod.txt` depend on `prod.in`
-------------------------------------

Let's inform `make` that `prod.txt` depends on `prod.in`:

<pre><code>requirements/prod.txt: <b>requirements/prod.in</b> $(pip_compile)
	$(pip_compile) requirements/prod.in > /dev/null 2>&1</code></pre>

As before, if you re-run `make` it won't remake `prod.txt` because it sees that
the file already exists and is newer than `prod.in`:

```terminal
$ make requirements/prod.txt
make: 'requirements/prod.txt' is up to date.
```

But now since `make` knows that `prod.txt` depends on `prod.in` if you change
`prod.in` then `make requirements/prod.txt` will remake `prod.txt` from the new
`prod.in`:

```terminal
$ echo flask >> requirements/prod.in  # Add a new dependency to prod.in.
$ make requirements/prod.txt
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/prod.in > /dev/null 2>&1
$ grep ^flask requirements/prod.txt  # flask has now been added to prod.txt.
flask==2.2.2
```

Making all the requirements files
---------------------------------

We could add more rules to our `Makefile` to tell `make` how to compile all our requirements files:

```make
# Makefile
requirements/prod.txt: requirements/prod.in $(pip_compile)
	$(pip_compile) requirements/prod.in > /dev/null 2>&1

requirements/dev.txt: requirements/dev.in $(pip_compile)
	$(pip_compile) requirements/dev.in > /dev/null 2>&1

requirements/format.txt: requirements/format.in $(pip_compile)
	$(pip_compile) requirements/format.in > /dev/null 2>&1
```

This works but we need a rule for each requirements file (seven in total for a
typical Hypothesis project). It'd be nicer if we could avoid all the
repetition. With a couple of `make` tricks we can write a single rule for
making any requirements file:

```make
# Makefile
requirements/%.txt: requirements/%.in $(pip_compile)
	$(pip_compile) $< > /dev/null 2>&1
```

There's two new `make` things here:

1. The `%` in `requirements/%.txt` is a wildcard that tells `make` that this is
   a recipe for making any `requirements/%.txt` file. The `requirements/%.in`
   tells `make` that each `requirements/%.txt` file depends on its
   corresponding `requirements/%.in` file of the same name.
2. The `$<` is a `make` [automatic variable](https://www.gnu.org/software/make/manual/make.html#Automatic-Variables)
   that resolves to the name of the target's first prerequisite (for example
   `requirements/prod.in`).

Now you can run `make requirements/<FILE>.txt` to compile any requirements file
as long as a corresponding `requirements/<FILE>.in` file exists.

Informing `make` of the dependencies between requirements files
---------------------------------------------------------------

`requirements/*.in` files can depend on other `requirements/*.txt` files with
`-r` lines.  For example `tests.in` depends on `prod.txt`:

    # tests.in
    pip-tools
    coverage
    httpretty
    ...
    -r prod.txt

When you compile `tests.in` the resulting `tests.txt` will contain everything
from `prod.txt` plus the dependencies from `tests.in` itself.

`lint.in` depends on `tests.txt` (and so indirectly on `prod.txt` as well) and
on `functests.txt`:

    # lint.in
    pip-tools
    pylint
    ...
    -r tests.txt
    -r functests.txt

These dependencies have a number of consequences for compiling requirements
files:

**Requirements files need to be compiled in the right order**. To compile
`tests.txt` first `prod.txt` needs to exist, otherwise `pip-compile` will
crash. To compile `lint.txt` first `prod.txt`, `tests.txt` and `functests.txt`
all need to exist.

**When you recompile a requirements file you need to recompile the others that
depend on it, in the right order**. If you change `tests.in` you need to first
recompile `tests.txt` and then also recompile `lint.txt` (in that order)
because `lint.txt` will also change as a resul of changing `tests.in`. If you
change `prod.in` you need to recompile first `prod.txt` then `tests.txt` (and
others that directly depend on `prod.txt`) and then `lint.txt` (and others that
depend on `tests.txt` or on other files that you've recompiled), all in the
right order.

`make` will do all this for us if we only inform it of the dependencies between
our requirements files, which we can do by adding a few more lines to
`Makefile`:

```makefile
# Makefile
# Save the paths to pip and pip-compile in variables.
python_bin_dir := $(shell pyenv root)/versions/$(shell pyenv local)/bin
pip := $(python_bin_dir)/pip
pip_compile := $(python_bin_dir)/pip-compile

# Tell make how to install pip-compile.
$(pip_compile):
	$(pip) install pip-tools > /dev/null 2>&1

# Tell make how to make requirements files.
requirements/%.txt: requirements/%.in $(pip_compile)
	$(pip_compile) $< > /dev/null 2>&1

# Inform make of the dependencies between our requirements files.
requirements/checkformatting.txt: requirements/format.txt
requirements/dev.txt: requirements/prod.txt
requirements/tests.txt: requirements/prod.txt
requirements/functests.txt: requirements/prod.txt
requirements/lint.txt: requirements/tests.txt requirements/functests.txt
```

`make` figures out the transitive dependencies for us. We've only listed each
requirements file's direct dependencies in the `Makefile`. `lint.in` contains
`-r tests.txt` and `-r functests.txt` so we've informed `make` of those direct
dependencies with:

```make
requirements/lint.txt: requirements/tests.txt requirements/functests.txt
```

Since we've also informed make that `tests.txt` and `functests.txt` both depend
on `prod.txt` `make` will infer that `lint.txt` depends indirectly on
`prod.txt`.

Now whenever you ask `make` to remake a requirements file it'll first remake
any files that it directly or indirectly depends on (if they need to be remade)
in the right order:

```terminal
$ make --assume-new requirements/prod.in requirements/lint.txt
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/prod.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/tests.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/functests.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/lint.in > /dev/null 2>&1
```

Making all the requirements files in a single command
-----------------------------------------------------

We have to type a really long command to make all the requirements files:

```terminal
$ make requirements/prod.txt requirements/dev.txt requirements/format.txt requirements/lint.txt requirements/tests.txt requirements/coverage.txt requirements/functests.txt
```

It'd be nice if there was a simple command that could (re-)make all the
`requirements/*.txt` files at once. Of course you could use a shell wildcard:

```terminal
$ make requirements/*.txt
```

But that only works if all the `*.txt` files are present. It won't work if one
or more of them don't exist yet. You could use the shell to generate the list
of `*.txt` files that ought to exist based on the `*.in` files that're present.
But we can make it easier by implementing a `make requirements` command that
does that for you...

### Phony targets

So far we've seen how to use `make` to run a recipe for creating a file and to
only re-run the recipe if the file is missing or if one of its prerequisites
has changed. It's also possible to get `make` to simply run a command (or
series of commands) that isn't intended to create a file. This is called a
[phony target](https://www.gnu.org/software/make/manual/make.html#Phony-Targets)
in `make`'s terms. The canonical example is a `make clean` command that cleans
up temporary and generated files so that you can start over:

```make
# Makefile
.PHONY: clean
clean:
	rm -f foo.txt bar.txt baz.txt
```

Running `make clean` will delete `foo.txt`, `bar.txt` and `baz.txt`:

```terminal
$ make clean
rm -f foo.txt bar.txt baz.txt
```

The `.PHONY: clean` tells `make` not to look at the mtimes of any prerequisite or
target files but to just always run the recipe regardless (otherwise the
existence of a file called `clean` would cause `make clean` to do nothing).

### `make requirements`

We can implement a command for making all the requirements files at once by
adding a phony `requirements` target to our `Makefile`:

```make
# Makefile
# Save the paths to pip and pip-compile in variables.
python_bin_dir := $(shell pyenv root)/versions/$(shell pyenv local)/bin
pip := $(python_bin_dir)/pip
pip_compile := $(python_bin_dir)/pip-compile

# Tell make how to install pip-compile.
$(pip_compile):
	$(pip) install pip-tools > /dev/null 2>&1

# Tell make how to make requirements files.
requirements/%.txt: requirements/%.in $(pip_compile)
	$(pip_compile) $< > /dev/null 2>&1

# Inform make of the dependencies between our requirements files.
requirements/checkformatting.txt: requirements/format.txt
requirements/dev.txt: requirements/prod.txt
requirements/tests.txt: requirements/prod.txt
requirements/functests.txt: requirements/prod.txt
requirements/lint.txt: requirements/tests.txt requirements/functests.txt

# Make `make requirements` recompile *all* the requirements files.
.PHONY: requirements
requirements: requirements/prod.txt requirements/dev.txt requirements/format.txt requirements/lint.txt requirements/tests.txt requirements/coverage.txt requirements/functests.txt
```

Now you can just run `make requirements` and it'll compile all the requirements
files that need to be recompiled, in the right order, without unnecessarily
recompiling anything that it doesn't need to.

`make` is still aware of the dependencies between files so for example if
`prod.in` has changed it'll recompile `prod.txt` and everything that depends on
it in the right order:

```terminal
$ make --assume-new requirements/prod.in requirements
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/prod.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/dev.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/tests.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/functests.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/lint.in > /dev/null 2>&1
```

You can use `--always-make` to force it to recompile everything:

```terminal
$ make --always-make requirements
/home/seanh/.pyenv/versions/3.10.4/bin/pip install pip-tools > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/prod.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/dev.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/format.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/tests.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/functests.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/lint.in > /dev/null 2>&1
/home/seanh/.pyenv/versions/3.10.4/bin/pip-compile requirements/coverage.in > /dev/null 2>&1
```

It's a shame to have to list the names of all the `requirements/*.txt` files in
the `Makefile`.  If we add or remove `*.txt` files we'll have to update this
list.  If different projects have different `*.txt` files they'll need
different `Makefile`'s.  Can `make` figure out which `requirements/*.txt` files
ought to exist by looking at the `requirements/*.in` files? Yes:

```make
.PHONY: requirements
requirements: $(foreach file,$(wildcard requirements/*.in),$(basename $(file)).txt)
```

This uses three `make` [functions](https://www.gnu.org/software/make/manual/make.html#Functions):

* `$(wildcard requirements/*.in)` returns a list of all the `requirements/*.in` files:
  `requirements/prod.in requirements/dev.in ...`

* `$(basename $(file))` strips the extension from a path:
  `requirements/prod.in` &rarr; `requirements/prod`.

  `$(basename $(file)).txt` changes the extension to `.txt`:
  `requirements/prod.in` &rarr; `requirements/prod.txt`.

* `$(foreach file,<FILES>,<FUNCTION>)` calls `<FUNCTION>` for each file
  in `<FILES>` and returns the list of values that `<FUNCTION>` returned.

  So the full expression `$(foreach file,$(wildcard
  requirements/*.in),$(basename $(file)).txt)` evaluates to the list of
  `requirements/*.txt` files that _ought to_ exist by listing the
  `requirements/*.in` files and replacing each file's `.in` extension with
  `.txt`. This works whether the `requirements/*.txt` files already exist (and
  are being recompiled) or don't exist yet (because they're being compiled for
  the first time from the `*.in`'s).

Making requirements files in parallel
-------------------------------------

Now that we've done all the work we can reap the benefits by having `make`
speed things up by parallelising the work for us. `make -j` will run the
recompilations in as many parallel jobs as possible. On my machine `make
--always-make requirements` takes 10s to recompile all the files. `make -j
--always-make requirements` does the same work in parallel in 4.5s.

`make` is aware that compiling all the requirements files depends on
`pip-compile` so it knows that it needs to finish installing `pip-tools` before
it can start any parallel `pip-compile` jobs. And it still accounts for the
dependencies between files, for example it won't start compiling `lint.txt`
until it has finished compiling all the other `*.txt`'s that `lint.txt`
directly or indirectly depends on.

Upgrading packages
------------------

To add or remove a dependency you would add the dependency to or remove it
from the appropriate `*.in` file(s) then just run:

```terminal
$ make -j requirements
```

But what about upgrading dependencies to newer versions? `pip-compile` can
upgrade all the dependencies in a requirements file to their latest versions
with:

```terminal
$ pip-compile --upgrade requirements.in
```

To upgrade just one package to its latest version:

```terminal
$ pip-compile --upgrade-package <FOO> requirements.in
```

To upgrade or downgrade a package to a specific version:

```terminal
$ pip-compile --upgrade-package <FOO>==<X.Y.Z> requirements.in
```

See [Updating requirements](https://pip-tools.readthedocs.io/en/latest/#updating-requirements)
in `pip-tools`'s docs for more details.

We can enable `pip-compile`'s `--upgrade` and `--upgrade-package` to be used by
our `Makefile` by adding a `$(args)` variable for passing command line
arguments through to `pip-compile`:

```make
# Tell make how to make requirements files.
requirements/%.txt: requirements/%.in $(pip_compile)
	$(pip_compile) $(args) $< > /dev/null 2>&1
```

Now to upgrade a dependency to the latest version in all requirements files
that it appears in:

```terminal
$ make -j requirements --always-make args='--upgrade-package <FOO>'
```

To upgrade or downgrade a package to a particular version:

```terminal
$ make -j requirements --always-make args='--upgrade-package <FOO>==<X.Y.Z>'
```

To upgrade *all* packages to their latest versions use `pip-compile`'s `--upgrade`:

```terminal
$ make -j requirements --always-make args=--upgrade
```

You can also use `args` with `make requirements/foo.txt` if you only want to
upgrade or downgrade a requirement (or all requirements) in just one (or more)
particular requirements files.

Telling `make` how to discover the dependencies for itself
----------------------------------------------------------

It's a shame to have to specify all the requirements file dependencies in the
`Makefile`:

```make
# Inform make of the dependencies between our requirements files.
requirements/checkformatting.txt: requirements/format.txt
requirements/dev.txt: requirements/prod.txt
requirements/tests.txt: requirements/prod.txt
requirements/functests.txt: requirements/prod.txt
requirements/lint.txt: requirements/tests.txt requirements/functests.txt
```

We might have to update those lists if the `*.in` files change and
the `Makefile` may need to differ between projects if the projects have
different requirements file dependencies. Can't `make` figure out the
dependencies itself by looking for the `-r` lines in the `*.in` files? Yes:

```make
# Makefile
# Save the paths to pip and pip-compile in variables.
python_bin_dir := $(shell pyenv root)/versions/$(shell pyenv local)/bin
pip := $(python_bin_dir)/pip
pip_compile := $(python_bin_dir)/pip-compile

# Tell make how to install pip-compile.
$(pip_compile):
	$(pip) install pip-tools > /dev/null 2>&1

# Tell make how to make requirements files.
find_prerequisites = $(shell sed -n 's/^[[:blank:]]*\(-r\|-c\)[[:blank:]]*\([[:alnum:]\/-_\.]\+\)/requirements\/\2/p' "requirements/$*.in" | tr '\n' ' ')
.SECONDEXPANSION:
requirements/%.txt: requirements/%.in $$(find_prerequisites) $(pip_compile)
	$(pip_compile) $(args) $< > /dev/null 2>&1

# Make `make requirements` recompile *all* the requirements files.
.PHONY: requirements
requirements: $(foreach file,$(wildcard requirements/*.in),$(basename $(file)).txt)
```

Okay now this is getting complicated. Regexes! I'm going to leave it as an
exercise for the reader to figure out exactly how this last part works, but
I'll give some hints:

* `find_prerequisites` shells out to [sed](https://www.gnu.org/software/sed/manual/sed.html)
  to find all the direct dependencies of a `requirements/*.in` file from the
  `-r` lines in the file.

  It uses a `sed` command of the form:

  ```bash
  sed -n 's/<regex>/<replacement>/p' | tr '\n' ' '
  ```

  * The `-n` tells sed *not* to print the non-matching file lines
  * The `/p` tells sed to print the matching lines
  * The regex part matches `'-r foo.txt'` or `'-c bar.txt'` lines
  * The replacement part prints `'requirements/foo.txt'` without the `-r` or `-c`,
    whitespace, or comments
  * The `$*` in `"requirements/$*.in"` is a Make automatic variable that resolves
    to the "stem" (just the `%` part) of the `requirements/%.txt` file being
    compiled.
  * The `tr` command at the very end joins the output onto one line
  * `sed` only needs to find the direct dependencies.
    `make` will then figure out the transitive dependencies for itself.

  You can test the `sed` command in a terminal. For example (replacing `$*` with `lint.in`):

  ```terminal
  $ sed -n 's/^[[:blank:]]*\(-r\|-c\)[[:blank:]]*\([[:alnum:]\/-_\.]\+\)/requirements\/\2/p' requirements/lint.in | tr '\n' ' '
  requirements/tests.txt requirements/functests.txt
  ```

* To get `$*` to work in the prerequisites of a rule we've had to enable
  `make`'s [secondary expansion](https://www.gnu.org/software/make/manual/make.html#Secondary-Expansion)
  feature. Also see [How `make` Reads a Makefile](https://www.gnu.org/software/make/manual/html_node/Reading-Makefiles.html).

Wrapping up
-----------

Here's the final `Makefile`:

```make
# Makefile
# Save the paths to pip and pip-compile in variables.
python_bin_dir := $(shell pyenv root)/versions/$(shell pyenv local)/bin
pip := $(python_bin_dir)/pip
pip_compile := $(python_bin_dir)/pip-compile

# Tell make how to install pip-compile.
$(pip_compile):
	$(pip) install pip-tools > /dev/null 2>&1

# Tell make how to make requirements files.
find_prerequisites = $(shell sed -n 's/^[[:blank:]]*\(-r\|-c\)[[:blank:]]*\([[:alnum:]\/-_\.]\+\)/requirements\/\2/p' "requirements/$*.in" | tr '\n' ' ')
.SECONDEXPANSION:
requirements/%.txt: requirements/%.in $$(find_prerequisites) $(pip_compile)
	$(pip_compile) $(args) $< > /dev/null 2>&1

# Make `make requirements` recompile *all* the requirements files.
.PHONY: requirements
requirements: $(foreach file,$(wildcard requirements/*.in),$(basename $(file)).txt)
```

Without the comments that's 14 lines of `Makefile`.
Those 14 lines pack quite a punch:

✓ Finds the correct Python version from your `.python-version` file  
✓ Installs `pip-tools`, avoids reinstalling it if it's already installed  
✓ Creates or updates `requirements/*.txt`'s from any set of `*.in`'s  
✓ Compiles files in the right order (according to the dependencies between them)  
✓ Avoids recompiling files that don't need to be recompiled  
✓ Upgrade or downgrade packages with `args=--upgrade` or `args=--upgrade-package ...`  
✓ Speed things up by compiling in parallel
